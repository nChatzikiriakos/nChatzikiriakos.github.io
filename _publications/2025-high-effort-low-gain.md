---
title: "High Effort, Low Gain: Fundamental Limits of Active Learning for Linear Dynamical Systems"
collection: publications
permalink: /publication/2025-high-effort-low-gain
excerpt: 'In this work, we consider the problem of identifying an unknown linear dynamical system given a finite hypothesis class. In particular, we analyze the effect of the excitation input on the sample complexity of identifying the true system with high probability. To this end, we present sample complexity lower bounds that capture the choice of the selected excitation input. The sample complexity lower bound gives rise to a system theoretic condition to determine the potential benefit of experiment design. Informed by the analysis of the sample complexity lower bound, we propose a persistent excitation (PE) condition tailored to the considered setting, which we then use to establish sample complexity upper bounds. Notably, the \acs{PE} condition is weaker than in the case of an infinite hypothesis class and allows analyzing different excitation inputs modularly. Crucially, the lower and upper bounds share the same dependency on key problem parameters. Finally, we leverage these insights to propose an active learning algorithm that sequentially excites the system optimally with respect to the current estimate, and provide sample complexity guarantees for the presented algorithm. Concluding simulations showcase the effectiveness of the proposed algorithm.'
date: 2025-09-15
link: 'https://www.arxiv.org/abs/2509.11907' 
github: 'https://github.com/col-tasas/2025-high-effort-low-gain'
citation: 'Chatzikiriakos, N., Jamieson, K., Iannelli, A. (2025) <i> arXiv preprint arXiv:</i> 2509.11907'
---